# djynkz
Proof-of-concept sampler code for future hardware sampler

I am designing a Electro-Smith Daisy ( https://electro-smith.com/collections/daisy ) based sampler and this is to help that project. It's quite a lot easier to develop and debug things on a computer than on a microcontroller.

Currently this is a primitive sampler with even more primeval sequencer for testing. It reads wav-files from directory given as first argument and creates to "samplers" from them. Wav-files can have smpl-chunk in the end ( https://www.recordingblogs.com/wiki/sample-chunk-of-a-wave-file ) and it contains midi note and loop point of that file. Also there is type of looping, this uses that with specific value 32 to tell that there is no looping. This should be changed that also having number of sample loops as zero would tell that this is not a looping sample. But the whole wav-reader is quite naive implementation and needs refining and testing with wav-files generated from multiple different sources. Also, as it can read multichannel files, the sampler itself doesn't understand that interleaved data, but I'm still not sure what to do with non-mono samples in the resulting hardware sampler. Even as it will have two outputs and you could have stereo-functionality, you don't usually want your samples to be in stereo files, as it is quite difficult to get stereo file to loop nicely, and it generally sounds better if you have dual mono files with different looping points playing as stereo. And if you haven't sampled all possible notes, it sounds even better if in different channels the samples are on different notes and stretched or shortened version plays in another channel while other has original sample. Some nineties samplers, like Korg N364 and N1, seem to do this.

But, to have that smpl-chunk, you can use this program to add that to a wave file having arguments 'path' 'filename' 'midinote' 'looppoint' 'looptype', where path is path of the sample, filename is filename, midinote is note of that sample, looppoint is point where the sample loops back then reaching end (I used Endless WAV https://www.bjoernbojahr.de/endlesswav.html to find suitable points), and looptype is the forementioned type of looping, when 0, or non-looping, when 32.

That
static uint16_t sampleBuffer[32000000];
is for Daisy-implementation where external 64 MB SDRAM that needs to be initialized in the beginning and all samples a written there sequentially and read with pointers, in a hopefully mustly footgunless way.

So, all wav-files are read and then put to suitable notes for currently two samplers, normal and staccato one. Staccato is just fancy way of saying non-looping samples, but it's handy concept for example palm muted notes with guitar, plucked violins, or just drums and percussion where you want just one OOMPH. Every note finds and adds sample with closest note for itself, in an ideal world, every note would have it's own custom sample. Technically you can have only one sample for whole range of 128 midi notes, but it probably sounds quite crude. Or if you like industrial, crude in the right way. If there are multiple samples for same note, they are all added to that note and played in a round-robin -way to have less machine like sound as the same sample doesn't just repeat. Possible improvement could be for every note to also take samples from couple of adjanced notes that don't sound too bad when pitched to have even more variety. This needs testing. Also, if note E2 is played and it is followed by F2, this needs to try not to play the same sample that was used for E2 for that F2. And if future hardware implementation has stereo or dual mono -out where both channels use the same sounds, it would be better if the separate channels never play the same sample at the same time, or even right after the other has played it. This is very fine tuning.

sequence.cpp contains array named "sequence" which is used to sequence the samples for testing purposes. Format is simple: array of structs that have 16th note tick when it happens, "true" or "false" for note to be played or stopped, midi note for that, and "true" for staccato and "false" for normal looping sample. And there are separate variables for tempo and sequence length. If the ticks when the sequence events happent aren't in numerical order, it breaks, sorry.

There is distortion engine and IR-based cabinet emulation for guitar sounds. In convolution the IR-sample can be stretched or shortened for different sounds. There is morphing function to slide between to different IR-samples. It basically creates an array of constant length vectors that follow the curves of the IR-sample and creates a 2D-matrix of it. The it can weighted average of two IR-samples in 2D-space and then convert it back to 1D-sample which can be used in the convolution. Idea here was to morph between IR-samples instead of just crossfading between them normally. Normal crossfade could introduce phase cancellations and this could give unique sound. In practice I didn't hear much difference between normal crossfading and this overly complicated 2D-morphing.

TODO:
- Use more modern C++ features, there are too many for-loops in cases where fancier solutions are available.
- I'm currently mostly proficient in writing TypeScript, there are probably stupid number types used here, it takes a while to get used to that there isn't just one "Number" -type that covers everything.
- The wav-reader is quite naive implementation and could find locations of the different chunks.
- Speaking of chunks, it now only reads Format, Data and Sample chunks. Instrument chunk could be useful, or even Cue chunk to have multiple samples in one wav-file.
- Better handling of keeping output signal in suitable range, some better limiter and volume handling so that peaks when playing chords don't get distorted.
- As I want this to also work as a metal-guitar-in-a-box in my modular synth, output needs somekind of guitar amp and speaker modeling. Update: work in progress.
- Better handling of errors and edge cases. With hardware sampler without display showing errors as text isn't that useful, so maybe having some kind of synthesized voice (or just samples...) reading out the errors in the audio output?
- Especially hard clipping is extremely aliasing, need to implement some kind of oversampling. There is implementation, needs testing if it works correctly. FIR-filters can be optimized for speedier performance in this case with some mathemancy
- Convolution and especially this IR-sample-morphing are extremely processor-intensive operations, using microcontroller for them can be problematic. Shortening IR-samples reduces load exponentially. Needs studying.